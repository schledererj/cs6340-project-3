{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, Load, and Visualize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by extracting the train-test data from the provided files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_file = np.load('kmnist-train-imgs.npz')\n",
    "train_images = train_images_file['arr_0']\n",
    "train_images_file.close()\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_file = np.load('kmnist-train-labels.npz')\n",
    "train_labels = train_labels_file['arr_0']\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_file = np.load('kmnist-test-imgs.npz')\n",
    "test_images = test_images_file['arr_0']\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_file = np.load('kmnist-test-labels.npz')\n",
    "test_labels = test_labels_file['arr_0']\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll spot-check the data by visualizing a few of the samples for the first 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0 = filter(lambda x: x[1] == 0, zip(train_images, train_labels))\n",
    "label_1 = filter(lambda x: x[1] == 1, zip(train_images, train_labels))\n",
    "label_2 = filter(lambda x: x[1] == 2, zip(train_images, train_labels))\n",
    "label_3 = filter(lambda x: x[1] == 3, zip(train_images, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
    "for ax in axes:\n",
    "    image = next(label_0)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image[0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % image[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAADCCAYAAABNCBjaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAW0ElEQVR4nO3de3DU5b3H8e9DCIRwTbiFm4KIMCVgaVVKKwJF26OCorVIRdCKcmSmKpS2ThFbazk6UNrKzDk6XCTgnVYR0aIIgqVUhWGKWhyulVsAy11uBgL8zh+k50D380B+64bNPvt+zThD3vyy+wvmt/my7LOPi6LIAAAAQlYj3ScAAABQ1Rh4AABA8Bh4AABA8Bh4AABA8Bh4AABA8Bh4AABA8Bh4kuCce9M5d0eqjwUyFdcEcCauierHZcv78DjnDp32Yb6ZHTWzExUf/2cURc+f/7NKHedcLTN7wcwuM7MLzaxPFEXvpvWkUK1xTQBn4poIW9Y8wxNFUb1//WdmW8ys/2nt/76JnXM103eWX9pSM7vdzD5L94mg+uOaAM7ENRG2rBl4fJxzvZ1zpc65B51zn5lZiXOuwDn3hnNul3NuX8WvW5/2Oe865+6u+PWdzrmlzrmJFcdudM5dm+Sx7ZxzS5xzB51zC51z/+Oce64yX0cURceiKHoiiqKl9v9/IwFi45oAzsQ1EYasH3gqFJlZoZ16im+4nfpzKan4+AIz+8LM/vssn9/dzNaaWRMzm2BmTzvnXBLHvmBmy82ssZk9YmZDTv9E59zHzrnbYn5tQDK4JoAzcU1kuEx+Wi6VTprZL6MoOlrx8Rdm9sq/ftM5919mtvgsn785iqKpFcfONLMnzay56acM5bEV/7Z6uZn1jaLomJktdc7NPf0ToyjqmswXBySBawI4E9dEhuMZnlN2RVFU9q8PnHP5zrnJzrnNzrkDZrbEzBo553I8n/9/37BRFB2p+GW9mMe2NLO9pzUzs60xvw4gVbgmgDNxTWQ4Bp5T/n2p2mgz62hm3aMoamBmV1V039OPqbDDzAqdc/mntTZVeH/A2XBNAGfimshwDDxafTv1dOV+51yhmf2yqu8wiqLNZrbCzB5xztVyzvUws/5xbsM5V9s5l1fxYS3nXN5Z/o0YiINrAjgT10SGYeDRnjCzOma228w+MLO3ztP9DjazHma2x8zGmdksO/U+EGZm5pz7xDk3+Cyfv9ZOXYCtzGx+xa8vrLKzRTZ5wrgmgNM9YVwTGSVr3ngwEznnZpnZmiiKqvxvDkAm4JoAzsQ1UXk8w1ONOOcud861d87VcM79h5ndaGZz0nxaQNpwTQBn4ppIHsvSq5ciM5ttp95fodTMRkRRtDK9pwSkFdcEcCauiSTxT1oAACB4/JMWAAAIHgMPAAAI3rlew5Ox/961adMm2ceOHSv75s2bZZ8zZ47sjRs3Tua0KmXevHmyX3/99bK3atVK9tLS0pSdUzWS7veLyNhrwsf3z9pZ8tYcIUj3/6jzfk0cPnxY9ssuu0z2NWvWyP61r31N9hUrVsge4jWxb98+2VetWiX7tm3bZO/Xr5/s9er53ky6Ssn/UTzDAwAAgsfAAwAAgsfAAwAAgsfAAwAAghfsGw82bdpU9iNHjsi+dOlS2X/0ox/J/uKLLyZ3YpXwhz/8Idbx9evXr6IzQUh8L4YvKCiQvUePHlV5OkDSlixZIrvvxck1a+ofdVOmTJE9xBcn+/heoO372bdu3TrZfS8YnzhxYkLr1atXJc8utXiGBwAABI+BBwAABI+BBwAABI+BBwAABI+BBwAABO9cu6UH9zb6ZWVlsnfr1k32DRs2yP7RRx/J/pWvfKXS57J+/XrZu3TpUunbMPOvMPO9aj7DpXv5RLW/Jj777DPZBwwYIPvUqVNlj/t9GMfx48dlP3bsmOz5+flVdi4ByLpr4sEHH5R9woQJsl955ZWy+1Z7hbhKa+vWrbL37t1b9k8//TQl99uoUaOEtnLlSnls27ZtU3KfxtYSAAAgWzHwAACA4DHwAACA4DHwAACA4DHwAACA4AW7l5ZPXl6e7I888ojsgwYNkv3RRx+V/aWXXkpoJ06ckMc+8MADsh89ejTW8YGuxkKSfHvjLFu2TPYxY8bI/tBDD8neokUL2evUqSO7Wu0xatQoeey+fftkHz58uOwjR46UvUYN/i4Xsi1btsQ6/uqrr5Y9XauxDh8+nNA+//xzeWxRUZHsvu9x3zV0ww03yO7bi9G3l1ZJSYns6msyM9u/f39C8+3HlcJVWhKPCgAAIHgMPAAAIHgMPAAAIHgMPAAAIHgMPAAAIHhZt5eWT3l5uexdu3aVfc2aNbLPnj07ofn2JPnJT34ie3Fxsey+PbMaNmwoe6DSvclNtb8mNm7cKLtvNd/evXtj3X7t2rVlr1Wrluxq1aFvzywf3wow3/5AjRs3jnX7GS7oa6K0tDShXXrppfLYL774Qvbq9n2iVmRNmTJFHjts2DDZfdf5fffdJ3uzZs1knz59uuxqDywz/z5mEydOlF155plnZB8yZEilb+Mc2EsLAABkJwYeAAAQPAYeAAAQPAYeAAAQvKzbWsInNzdX9gkTJsjue5vuESNGJDTfW4Z36tRJ9sWLF8ueZS9ORpJatWole4cOHWT3bTnh49v6xNfj6Nixo+y+F/hn2YuTs9LatWsTmu+F9r7H1Or2fVKvXr2E1rx5c3nsokWLZE/Vi5MLCwtl90nFdhy+r7Wq8QwPAAAIHgMPAAAIHgMPAAAIHgMPAAAIHgMPAAAIHqu0zqFfv36yX3PNNbIvWLCg0rc9d+5c2Zs0aVLp2wD+nW+Lh5kzZ8p+yy23yL5q1aqUnI9akaK2YDEz69u3r+w1avB3s2z12muvVfrYtm3bVt2JpJDaWsW3vZFvK4f9+/fL/txzz8kedzXW+vXrZZ86dWqs2ykoKEhovm1uqhqPIgAAIHgMPAAAIHgMPAAAIHgMPAAAIHgMPAAAIHis0joH3yvV1f4ucb3zzjuy+/Y8Ar4M3z5VvlUwffr0kX3Lli2y5+Xlyf6zn/0soX31q1+Vx6Zinx5kpu3bt8vuW9GnfPLJJ7I/8MADsvtWQLVs2bLS95kM9fPDtzdWWVmZ7CNHjpT929/+dtLndbq//vWvsvtWh9WvX1/2SZMmJbS4K8ZShWd4AABA8Bh4AABA8Bh4AABA8Bh4AABA8Bh4AABA8FwURWf7/bP+ZkjmzJkj+/Dhw2XftWvXl77P3Nxc2efPny+7b9VMlkn3Mp7gronNmzfL7tvvpmvXrrL/8Ic/lH3EiBEJrWHDhvLY6667Tvb7779f9uLiYtmzTBDXxM6dO2Vv3759Qjt06FAq7tJmzZol+8CBA1Ny+ydOnJD9nnvuSWglJSXyWN+Kxvfff19232pJH9/PMt/PG98K5YkTJ8quVp+dh73x5DXBMzwAACB4DDwAACB4DDwAACB4DDwAACB4DDwAACB4rNKqsHXrVtkvueQS2X37m6RC8+bNZZ85c6bs3/3ud6vsXKqhIFakpMOxY8dk9+3J8/rrr8v+wQcfyP7nP/9Z9sGDB5/75M7Bt6rr1Vdflb13796yB7pXV7q/qJRcE77H1FtuuSWhzZs3Tx7r2y9uzZo1sjdo0ED2DRs2yN60aVPZfec+ZMgQ2V9++eWEVrOm3tpy8uTJst91112y+/hWjH3/+9+X3bdy+fbbb5d9xowZsp+HFVkKq7QAAEB2YuABAADBY+ABAADBY+ABAADBY+ABAADB0y8Lz0Jt2rSR3bc/0LJly2QvLy9PaPv27ZPHlpaWyv7Pf/5T9tmzZ8ueZau0cA6+lZcvvPCC7NOnT5f9tddek71Vq1ayf/7555U4u+T4bvvee++VfeXKlbLn5+en7JyQWr49oNTj3vLly+Wxvv7zn/9c9gMHDsj+yiuvyH7bbbfJPmjQINnffPNN2ZUOHTrIXlhYWOnbMPOvxvStrlywYIHso0aNkv3RRx+VPU2rsWKp/mcIAADwJTHwAACA4DHwAACA4DHwAACA4DHwAACA4LGXVhoNHTpU9meffVZ23z4x77//vuwFBQXJnVj1FsS+Qenw2GOPye7bq2f16tWy+1Y6+fY36t+/f0I7efKkPNbHtwLkyiuvlH3hwoWy5+bmxrrfDME1UUGtkjUzu/XWW2X37cXm27vN9xjsWx3mox6bn3rqKXns5ZdfLvtFF10ku2+V1lVXXSX7N7/5Tdl/+9vfyp4h+9GxlxYAAMhODDwAACB4DDwAACB4DDwAACB4DDwAACB47KWVJN9qgJycnITmW2Hyu9/9TnbfHkZr166V/f7775fdt9oLYfOtvNyzZ4/stWvXjnU7Pr6VV3FXZMW5jc6dO8tesyYPbdnItwrvBz/4gey+VVq+vdvirsbyGTZsWELzfY83adIk1m379u86evSo7L69sTJkNVYsPMMDAACCx8ADAACCx8ADAACCx8ADAACCx8ADAACCx1KGJI0fP172J598MqF16tRJHuvbI8W3V9GBAwdk/+Mf/yj7tGnTZPetykEYfKsrduzYIfumTZtkHzFihOytW7eW/cUXXzz3ySWpTp06sv/0pz+VPcQVJkjexRdfnJb7veOOO2RX+yi2bNlSHlu3bl3ZT5w4IfukSZNkf/rpp2WvV6+e7CHiGR4AABA8Bh4AABA8Bh4AABA8Bh4AABA8XrScpLFjx8qu3jL8N7/5jTzW9+Iy31uA+/iO//DDD2Xv3r17rNtHGBo3biy7b5uU6rQ1ScOGDWXPy8s7z2eCTLR379603O+tt94qu1o44nsc921NtHjxYtmbN28ue7du3WTPJjzDAwAAgsfAAwAAgsfAAwAAgsfAAwAAgsfAAwAAgueiKDrb75/1N/HlrF+/XvbHH39c9pKSEtmLi4tlX7JkiewFBQWVOLtqK917BmTsNTF//nzZr732WtnP8dhQLTz11FOy33vvvef5TNKKa+IcVqxYIfsVV1whe9zv/RYtWsjuW0nVtGnThNaoUSN5rG+blO9973uy+7Zb6dGjh+yBkn9oPMMDAACCx8ADAACCx8ADAACCx8ADAACCx8ADAACCx15aadShQwfZ77rrLtl9q7Rat24te4avxkKK9ezZU/ZevXrJvnTpUtmPHz8uu281SbNmzRJaYWGhPHb16tWy+4wbN072G2+8UXbfahqEraysTPa4q7F8e7dNmzZN9rp168pes2bij17fnlmHDx+W3bfKt2vXrrKDZ3gAAEAWYOABAADBY+ABAADBY+ABAADBY+ABAADBY5VWNbRr165Yx5eWllbRmSAk+fn5sv/pT3+S/e2335bdt7IlNzdX9u7duye08vJyeaxvhaJvH7Bt27bJ/tJLL8k+cuRI2X0rzBAG355Wcd10002y+/aj27Rpk+x/+9vfElrv3r3lsVu2bJH9kksukd13nYNneAAAQBZg4AEAAMFj4AEAAMFj4AEAAMFj4AEAAMHLulVakyZNkn38+PGyFxcXy37nnXfKfvPNNyc03/4rPvPmzYt1fLt27WT3raZhRQpO51vVMWDAgPN7ImY2Y8YM2b/xjW/IvnnzZtkfeugh2S+++GLZ+/fvL/uJEydkz8nJkR3VU9yVrLVr15Z9zJgxsvseU+vVqyf7t771rUqfi28V5Zw5c2RftWqV7F26dKn0fYaKZ3gAAEDwGHgAAEDwGHgAAEDwGHgAAEDwGHgAAEDwgl2ltX//ftkffvhh2WvU0LNfw4YNY93OsGHDEtqFF14oj+3QoYPsCxYskN232mvgwIGy79ixQ/aWLVvKDqRbUVGR7K+++qrsV111leyHDh2S/b777pPdt8qmX79+sqN62r59u+y+feF8LrjgAtk7d+4c63bq1q0r+5EjRxKa72dNQUGB7L6fWS+//LLsrNLiGR4AAJAFGHgAAEDwGHgAAEDwGHgAAEDwGHgAAEDwnG+/pQpn/c3qbMmSJbL36tVL9pEjR8r++9//Xvby8nLZJ0+enNBGjRoljz1+/LjsPjNnzpR96NChsW4nw6V7I7CMvSaqk1Tt8+b73n/22Wdj3U7Pnj1lnz17tuxNmjSJdftVjGuigu/x+sc//nGs2/n1r38t+9ixY2Ofk3Ly5MmE5lt1deDAAdm//vWvy+7bv8u3eqt9+/ayZzh5TfAMDwAACB4DDwAACB4DDwAACB4DDwAACB4DDwAACF6wq7R2794te9OmTWW/6aabZPet0ojjww8/lH3WrFmy9+jRQ/YbbrjhS59LAFiRUk0dPHhQ9sceeyyh5eTkyGPHjRsX6z59qzH79Okju1odczbjx4+XffTo0bL7vq4qxjVRoW/fvrIvWrRI9lq1asm+du1a2du2bZvUef27v//97wnNt3+Xb48t3z5vvq/VtwpswIABsg8aNEh2389Q36qxmjXTsmUnq7QAAEB2YuABAADBY+ABAADBY+ABAADBY+ABAADBC3aVlo9vBdTy5ctl/+ijj2QvLi5O2TkhFlakVFNlZWWyqxVT119/vTx2zJgxsu/cuTPW8SUlJbLH1bx5c9nfe+892S+66KKU3G9MWXdN+B6Xr7jiCtmPHTsmu+/78I033kjuxCppy5YtCa127dryWN/3oO9n1sCBA2XfunWr7HFXLvr2uxsyZIjsEydOTGi+lV4pxCotAACQnRh4AABA8Bh4AABA8Bh4AABA8Bh4AABA8LJulZZvr56HH35Y9s6dO8v++uuvy96uXbvkTgyVlXUrUo4fPy776tWrZV+3bp3sGzdulN23emPZsmWyd+3aVfbu3bvLPnbs2ISWl5cnjy0qKpJd7T1kZnb48GHZU8W319LChQtl79mzZ1Wejk/WXRO+fQV9j8u+/ZxWrlwpezpW4ZaXl8uem5sru2911dy5c2V//vnnZf/LX/4i+969e2X3rdLyPU7dfPPNCc23j6Rvv68ksEoLAABkJwYeAAAQPAYeAAAQPAYeAAAQvKx70fL27dtl970l+bZt22Rv1qyZ7OqFYVdffXUlzw6VkHUv0PS9jX7v3r1l379/f9WdTBLUCxF9Lwb2vfAxJydH9qNHjyZ/YpXQrVs32Z955hnZ07TlTNDXhNoWon379vLY0tJS2b/zne/I/tZbb8nue2FuJvO9KPof//iH7O+++67s+fn5ss+YMUP2e+65J6ENGjRIHpvCP3detAwAALITAw8AAAgeAw8AAAgeAw8AAAgeAw8AAAiefr/tgLVs2VL2t99+W3bfSpidO3fKft111yW0YcOGyWMff/xx2Rs1aiQ7spNapWJW/VZj+bZVmTlzZkJr06aNPNa3ivLgwYOy+94u/+OPP5Z97dq1svvs3r1b9rp168a6HSRPbR+yZ8+eWLcxYMAA2TNhNZZvJXXcc/dtUdGpU6dY3Wfo0KGxjk8HnuEBAADBY+ABAADBY+ABAADBY+ABAADBY+ABAADBy7q9tOLyrfbo2bOn7AcOHKj0bf/qV7+S/Re/+IXsZWVlsk+ZMkX2Xr16yX7ppZdW4uyqrXQvqzjv18ShQ4dkHz16tOzTp0+X3bdPVark5eXJPmvWrITWv39/eWzclSe+vbR8fd26dbFuv0WLFrL7VnumadVP0NfEhg0bElrHjh3lsfXr15fd9zju+//rW9GEjMFeWgAAIDsx8AAAgOAx8AAAgOAx8AAAgOAx8AAAgOCxSitJ7733nuxjxoxJaJ9++qk8dvbs2bIXFRXJfs0118i+Zs0a2RcvXiy7b3+wDBH0ihR5h55r1Lfqatq0abKvX78+1u349qnau3ev7D6FhYUJ7Z133pHHdunSRfacnJxY95llgr4mFi1alNAGDx4sjx01apTsd999t+w1a+rtJBs0aFDJs0M1xSotAACQnRh4AABA8Bh4AABA8Bh4AABA8Bh4AABA8M61SgsAACDj8QwPAAAIHgMPAAAIHgMPAAAIHgMPAAAIHgMPAAAIHgMPAAAI3v8CcvMDdwrL3/AAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
    "for ax in axes:\n",
    "    image = next(label_1)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image[0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % image[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
    "for ax in axes:\n",
    "    image = next(label_2)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image[0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % image[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
    "for ax in axes:\n",
    "    image = next(label_3)\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image[0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % image[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the features\n",
    "\n",
    "We'll flatten the 2D 28 x 28 pixel arrays to 1D 784 pixel arrays. We could use scikit-learn's `StandardScaler` here to scale the values to have a 0 mean and 1 std. deviation, but Keras has a built-in scaler, so we'll include that as part of the deep learning model in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_flat = train_images.reshape(60000, 28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a multi-layer perceptron\n",
    "\n",
    "Using Keras, we'll build up a simple multi-layer perceptron model with an input layer, a scaling layer, 2 hidden dense layers, and an output dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_2 (Rescaling)     (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 6280      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                90        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,442\n",
      "Trainable params: 6,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Use a sequential, linear stack of layers\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add an input layer\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(28*28,)))\n",
    "\n",
    "# Add a scaler to scale the [0, 255] values to [0, 1]\n",
    "model.add(tf.keras.layers.Rescaling(scale=1./255))\n",
    "\n",
    "# Add 2 dense layers with 8 nodes each, with RELU activation functions\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# Add a dense layer with N nodes (N being the number of classes) and a softmax activation function\n",
    "num_classes = len(set(train_labels))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compile the model. The `compile` function takes input arguments for the loss function, the optimizer, and additional metrics to calculate along with loss. Since we're classifying with >2 labels and the labels are provided in integer format, we'll use the `SparseCategoricalCrossentropy` loss function. There are several options for the optimizer class, with no immediately apparent advantages or disadvantages over the others, so we'll use the one that was used in the MNIST tutorial, `RMSprop`. As for additional metrics, we'll take `accuracy`, which is more immediately indicative of the model's performance than loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_2 (Rescaling)     (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 6280      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                90        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,442\n",
      "Trainable params: 6,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a freshly-compiled model, we're ready to train! Fitting a deep learning model has more tunable parameters than a run-of-the-mill ML model, but we'll just be tuning the batch size and number of epochs to train over. For the first run, we'll just use a batch size of the default of 32, and 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 941us/step - loss: 1.0693 - accuracy: 0.6638\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 814us/step - loss: 0.6866 - accuracy: 0.7915\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 863us/step - loss: 0.6267 - accuracy: 0.8098\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 828us/step - loss: 0.5968 - accuracy: 0.8177\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 847us/step - loss: 0.5765 - accuracy: 0.8248\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 826us/step - loss: 0.5632 - accuracy: 0.8298\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 818us/step - loss: 0.5528 - accuracy: 0.8320\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 866us/step - loss: 0.5455 - accuracy: 0.8338\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 827us/step - loss: 0.5387 - accuracy: 0.8362\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 820us/step - loss: 0.5335 - accuracy: 0.8388\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=train_images_flat,\n",
    "    y=train_labels,\n",
    "    batch_size=32,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.88%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {(history.history['accuracy'][-1] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is decent accuracy for a first pass, but keep in mind it's only the accuracy of the final pass of _training_. We don't know how it will fare against the testing dataset. Let's play around with some of the parameters. First off, let's \"functionize\" the creation and training of the model, the predictions, and the accuracy calculation. I want to be able to pass the optimizer, the number of training epochs, the number of hidden dense layers, and the number of nodes in the hidden dense layers as inputs, and get an accuracy score against the testing images as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_flat = test_images.reshape(10000, 28*28)\n",
    "\n",
    "def compile_fit_model_and_predict(num_epochs, num_hidden_dense_layers, num_dense_layer_nodes, optimizer):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(28*28,)))\n",
    "    model.add(tf.keras.layers.Rescaling(scale=1./255))\n",
    "\n",
    "    for i in range(0, num_hidden_dense_layers):\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            num_dense_layer_nodes, activation='relu'))\n",
    "\n",
    "    # Add a dense layer with N nodes (N being the number of classes) and a softmax activation function\n",
    "    num_classes = len(set(train_labels))\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x=train_images_flat,\n",
    "        y=train_labels,\n",
    "        batch_size=32,\n",
    "        epochs=num_epochs,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    predictions = model.predict(test_images_flat)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    return {\n",
    "        'Number of epochs': num_epochs,\n",
    "        'Number of hidden layers': num_hidden_dense_layers,\n",
    "        'Number of nodes per hidden layer': num_dense_layer_nodes,\n",
    "        'Optimizer class': type(optimizer),\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, we'll try changing up the optimizer. It's difficult to tell which optimizer will perform the best just from the documentation, so we'll run a few experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers as op\n",
    "\n",
    "optimizers = [\n",
    "    op.SGD(),\n",
    "    op.Adam(),\n",
    "    op.Adadelta(),\n",
    "    op.Adagrad(),\n",
    "    op.Adamax(),\n",
    "    op.Nadam(),\n",
    "    op.Ftrl()\n",
    "]\n",
    "\n",
    "results = list()\n",
    "for _opt in optimizers:\n",
    "    results.append(compile_fit_model_and_predict(10, 2, 8, _opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of epochs</th>\n",
       "      <th>Number of hidden layers</th>\n",
       "      <th>Number of nodes per hidden layer</th>\n",
       "      <th>Optimizer class</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.nadam.Nadam'&gt;</td>\n",
       "      <td>0.7034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.7010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adamax.Adamax'&gt;</td>\n",
       "      <td>0.6919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.gradient_descent.SGD'&gt;</td>\n",
       "      <td>0.6743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adagrad.Adagrad'&gt;</td>\n",
       "      <td>0.4684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adadelta.Adadelta'&gt;</td>\n",
       "      <td>0.1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.ftrl.Ftrl'&gt;</td>\n",
       "      <td>0.1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of epochs  Number of hidden layers  \\\n",
       "5                10                        2   \n",
       "1                10                        2   \n",
       "4                10                        2   \n",
       "0                10                        2   \n",
       "3                10                        2   \n",
       "2                10                        2   \n",
       "6                10                        2   \n",
       "\n",
       "   Number of nodes per hidden layer  \\\n",
       "5                                 8   \n",
       "1                                 8   \n",
       "4                                 8   \n",
       "0                                 8   \n",
       "3                                 8   \n",
       "2                                 8   \n",
       "6                                 8   \n",
       "\n",
       "                                     Optimizer class  Accuracy  \n",
       "5           <class 'keras.optimizer_v2.nadam.Nadam'>    0.7034  \n",
       "1             <class 'keras.optimizer_v2.adam.Adam'>    0.7010  \n",
       "4         <class 'keras.optimizer_v2.adamax.Adamax'>    0.6919  \n",
       "0  <class 'keras.optimizer_v2.gradient_descent.SGD'>    0.6743  \n",
       "3       <class 'keras.optimizer_v2.adagrad.Adagrad'>    0.4684  \n",
       "2     <class 'keras.optimizer_v2.adadelta.Adadelta'>    0.1326  \n",
       "6             <class 'keras.optimizer_v2.ftrl.Ftrl'>    0.1000  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the `Adam` and `Nadam` optimizers outperform the others just slightly, at least with the current, fixed set of parameters, so let's stick with `Adam`. Now, let's generate some permutations of the other parameters to build and fit the model with, and then calculate the accuracy of their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "epochs_options = [25, 50]\n",
    "hidden_layers_options = [0, 1, 2]\n",
    "hidden_layer_nodes_options = [32, 64, 128]\n",
    "optimizer_options = [op.Adam()]\n",
    "\n",
    "results = list()\n",
    "for args in product(epochs_options, hidden_layers_options, hidden_layer_nodes_options, optimizer_options):\n",
    "    results.append(compile_fit_model_and_predict(*args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model, I recorded the accuracy of the predictions. The following table shows the accuracies for each model, with the most accurate on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of epochs</th>\n",
       "      <th>Number of hidden layers</th>\n",
       "      <th>Number of nodes per hidden layer</th>\n",
       "      <th>Optimizer class</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.9017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.8974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.8921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.8847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.8808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.8784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.8683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.8629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.8363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.8295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.8237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.8040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.6979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.6971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.6970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.6953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.6951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.6919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of epochs  Number of hidden layers  \\\n",
       "8                 25                        2   \n",
       "17                50                        2   \n",
       "5                 25                        1   \n",
       "14                50                        1   \n",
       "7                 25                        2   \n",
       "16                50                        2   \n",
       "4                 25                        1   \n",
       "13                50                        1   \n",
       "6                 25                        2   \n",
       "15                50                        2   \n",
       "3                 25                        1   \n",
       "12                50                        1   \n",
       "10                50                        0   \n",
       "11                50                        0   \n",
       "0                 25                        0   \n",
       "9                 50                        0   \n",
       "1                 25                        0   \n",
       "2                 25                        0   \n",
       "\n",
       "    Number of nodes per hidden layer                         Optimizer class  \\\n",
       "8                                128  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "17                               128  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "5                                128  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "14                               128  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "7                                 64  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "16                                64  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "4                                 64  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "13                                64  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "6                                 32  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "15                                32  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "3                                 32  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "12                                32  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "10                                64  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "11                               128  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "0                                 32  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "9                                 32  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "1                                 64  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "2                                128  <class 'keras.optimizer_v2.adam.Adam'>   \n",
       "\n",
       "    Accuracy  \n",
       "8     0.9017  \n",
       "17    0.8974  \n",
       "5     0.8921  \n",
       "14    0.8847  \n",
       "7     0.8808  \n",
       "16    0.8784  \n",
       "4     0.8683  \n",
       "13    0.8629  \n",
       "6     0.8363  \n",
       "15    0.8295  \n",
       "3     0.8237  \n",
       "12    0.8040  \n",
       "10    0.6979  \n",
       "11    0.6971  \n",
       "0     0.6970  \n",
       "9     0.6953  \n",
       "1     0.6951  \n",
       "2     0.6919  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the most accurate model is built with 2 hidden layers of 128 nodes (or _neurons_) each, and is trained on 25 epochs. Now, let's try the same experiments with a _convolutional_ neural network (CNN).\n",
    "\n",
    "## Implementing a Convolutional Neural Network\n",
    "\n",
    "Next up, we'll build a convolutional network using largely the same patterns as the multi-layer network. This time, instead of using the flattened, 1D images as inputs, we'll input the 2D arrays and then flatten after some 2D convolution and pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_134\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_378 (Dense)           (None, 128)               1179776   \n",
      "                                                                 \n",
      " dense_379 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_380 (Dense)           (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,216,394\n",
      "Trainable params: 1,216,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(28, 28, 1)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# use 2 dense layers with 128 nodes, per the most accurate perceptron model from the previous section\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=op.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 72s 151ms/step - loss: 0.9309 - accuracy: 0.8899\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 71s 151ms/step - loss: 0.0818 - accuracy: 0.9759\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 70s 149ms/step - loss: 0.0442 - accuracy: 0.9862\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 71s 151ms/step - loss: 0.0321 - accuracy: 0.9897\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 71s 152ms/step - loss: 0.0222 - accuracy: 0.9929\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 71s 151ms/step - loss: 0.0237 - accuracy: 0.9922\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 71s 150ms/step - loss: 0.0235 - accuracy: 0.9927\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 70s 150ms/step - loss: 0.0151 - accuracy: 0.9952\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 71s 150ms/step - loss: 0.0172 - accuracy: 0.9944\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 70s 149ms/step - loss: 0.0135 - accuracy: 0.9957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x218babf65b0>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_images, y=train_labels, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9289\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_images)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. We're already about 2% better than the perceptron with only the first pass! Since this took a long time to run (11 mins, compared to the perceptrons, which took ~15 sec), I'd like to increase the batch size to see how it impacts time to fit and the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 66s 278ms/step - loss: 1.1770 - accuracy: 0.8524\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 65s 275ms/step - loss: 0.0848 - accuracy: 0.9760\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 64s 274ms/step - loss: 0.0423 - accuracy: 0.9877\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 65s 275ms/step - loss: 0.0215 - accuracy: 0.9937\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 0.0166 - accuracy: 0.9947\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 65s 276ms/step - loss: 0.0131 - accuracy: 0.9956\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 0.0127 - accuracy: 0.9958\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 66s 281ms/step - loss: 0.0149 - accuracy: 0.9952\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 65s 277ms/step - loss: 0.0126 - accuracy: 0.9960\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 65s 275ms/step - loss: 0.0099 - accuracy: 0.9969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2186f98be50>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(28, 28, 1)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# use 2 dense layers with 128 nodes, per the most accurate perceptron model from the previous section\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "num_classes = len(set(train_labels))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=op.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x=train_images, y=train_labels, batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9335\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_images)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, neither the timing nor the accuracy improved, but they also didn't worsen. We'll stick with this one, though. Let's functionize all this such that we can easily pass in different parameters in an attempt to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import cpu_count\n",
    "\n",
    "def compile_fit_cnn_model_and_predict(num_epochs, num_filters, kernel_size, pooling, optimizer):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=(28, 28, 1)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=num_filters, kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=num_filters*2, kernel_size=kernel_size, activation='relu'))\n",
    "\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "    if pooling == 'global':\n",
    "        model.add(tf.keras.layers.GlobalMaxPooling2D())\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # use 2 dense layers with 128 nodes, per the most accurate perceptron model from the previous section\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "    num_classes = len(set(train_labels))\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer=op.Adam(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x=train_images,\n",
    "        y=train_labels,\n",
    "        batch_size=256,\n",
    "        epochs=num_epochs,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    predictions = model.predict(test_images)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    return {\n",
    "        'Number of epochs': num_epochs,\n",
    "        'Number of filters': num_filters,\n",
    "        'Kernel size': kernel_size,\n",
    "        'Pooling type': pooling or 'none',\n",
    "        'Optimizer class': type(optimizer),\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interest of time, we'll stick with the `Adam` optimizer to cut down on the number of permutations of the parameters. It's important to keep in mind, however, that varying the optimizer class is yet another dial one could turn to get better timing and/or accuracy of the model. Similarly, we're only going to test a set of 3 options for each of the other parameters so it doesn't take too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_options = [5, 10, 15]\n",
    "num_filters_options = [16, 32, 64]\n",
    "kernel_size_options = [3, 5, 7]\n",
    "pooling_options = ['max', 'global', 'none']\n",
    "optimizer_options = [op.Adam()]\n",
    "\n",
    "results = list()\n",
    "for args in product(epochs_options, num_filters_options, kernel_size_options, pooling_options, optimizer_options):\n",
    "    results.append(compile_fit_cnn_model_and_predict(*args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I had to kill this cell after running for 1,285 minutes, _but_ it had finished predictions on models using 76 of the possible 81 permutations of hyperparameters by that point. Let's take a look at those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of epochs</th>\n",
       "      <th>Number of filters</th>\n",
       "      <th>Kernel size</th>\n",
       "      <th>Pooling type</th>\n",
       "      <th>Optimizer class</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>max</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.9374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>max</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.9345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>max</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.9340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>max</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.9336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>max</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.9335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>global</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.7424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>global</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.7326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>global</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.7038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>global</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.6894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>global</td>\n",
       "      <td>&lt;class 'keras.optimizer_v2.adam.Adam'&gt;</td>\n",
       "      <td>0.6113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of epochs  Number of filters  Kernel size Pooling type  \\\n",
       "36                10                 32            3          max   \n",
       "48                10                 64            5          max   \n",
       "66                15                 32            5          max   \n",
       "39                10                 32            5          max   \n",
       "24                 5                 64            7          max   \n",
       "..               ...                ...          ...          ...   \n",
       "19                 5                 64            3       global   \n",
       "55                15                 16            3       global   \n",
       "10                 5                 32            3       global   \n",
       "28                10                 16            3       global   \n",
       "1                  5                 16            3       global   \n",
       "\n",
       "                           Optimizer class  Accuracy  \n",
       "36  <class 'keras.optimizer_v2.adam.Adam'>    0.9374  \n",
       "48  <class 'keras.optimizer_v2.adam.Adam'>    0.9345  \n",
       "66  <class 'keras.optimizer_v2.adam.Adam'>    0.9340  \n",
       "39  <class 'keras.optimizer_v2.adam.Adam'>    0.9336  \n",
       "24  <class 'keras.optimizer_v2.adam.Adam'>    0.9335  \n",
       "..                                     ...       ...  \n",
       "19  <class 'keras.optimizer_v2.adam.Adam'>    0.7424  \n",
       "55  <class 'keras.optimizer_v2.adam.Adam'>    0.7326  \n",
       "10  <class 'keras.optimizer_v2.adam.Adam'>    0.7038  \n",
       "28  <class 'keras.optimizer_v2.adam.Adam'>    0.6894  \n",
       "1   <class 'keras.optimizer_v2.adam.Adam'>    0.6113  \n",
       "\n",
       "[76 rows x 6 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).sort_values(by='Accuracy', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bf0a7eb58a0ecbe06ab574973ab3f2d2fa16eb819a3eb9f056d8b04161afe49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
